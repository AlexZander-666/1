%% The first command in your LaTeX source must be the \documentclass command.
%%
\documentclass[jec]{acnsart}

% 允许更灵活的断行以避免 overfull hbox
\tolerance=1000
\emergencystretch=3em
\hfuzz=2pt
\sloppy

% 抑制部分无害警告
\usepackage{silence}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{placeins}

% 让LaTeX更积极地放置浮动体
\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.7}

% 减少图表与正文之间的间距
\setlength{\textfloatsep}{6pt plus 2pt minus 2pt}
\setlength{\floatsep}{6pt plus 2pt minus 2pt}
\setlength{\intextsep}{6pt plus 2pt minus 2pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{2pt}
\graphicspath{{./figures/}{figures/}{./}}
\WarningFilter{hyperref}{Ignoring empty anchor}
\WarningFilter{scrartcl}{Incompatible usage}
\WarningFilter{pdfx}{Setting all color commands}
\WarningFilter{doclicense}{The pdfx package was detected}
\WarningFilter{mathdesign}{No font specified}
\WarningFilter{mathdesign}{`bookman' option not defined}

\setdoi{10.55056/jec.ARTNUM}
\setjournalyear{2025}
\setjournalvolume{1}
\setjournalissue{1}
\setcounter{page}{1}


%%
%% end of the preamble, the start of the body of the document source.

%%
%% The "title" command
\title{Physics-Guided Contrastive Learning for Low-Latency On-Device Wearable Fall Detection}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.

% Authors and affiliations removed for anonymous review
\author[1]{Anonymous Author(s)}
\address[1]{Anonymous Institution}

\begin{document}

%%
%% The abstract is a summary of the work to be presented in the
%% article.
\begin{abstract}
Wearable edge devices for fall detection must balance high sensitivity with stringent latency, memory, and energy constraints. PhyCL-Net addresses these through a time-domain architecture employing a Physics-Guided Dynamic Kernel (PDK) and Jerk-aware Fall-Aware Attention (FAA), eliminating spectral overhead. Training uses hierarchical dual-view contrastive regularization, with only the core model retained during deployment. On SisFall with 12-fold LOSO validation, PhyCL-Net achieves 98.21\% ${\pm}$ 0.10\% accuracy and 98.16\% ${\pm}$ 0.10\% Macro-F1 across five seeds, requiring 1.049M parameters and 0.15 GFLOPs, with median CPU latency reduced by 31.5\% to 125.99\,ms compared to the spectral baseline. PhyCL-Net achieves FPR of 0.72\% at 95\% TPR, with TPR decreasing by only 2.73 percentage points when FPR is constrained to 1\%, illustrating the accuracy--efficiency trade-off.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  Edge AI \sep
  Wearable Fall Detection \sep
  Physics-Inspired Deep Learning \sep
  Contrastive Learning \sep
  Real-Time Inference \sep
  Resource-Constrained Deployment
\end{keywords}

%%
%% This command processes the author affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Falls represent a leading cause of injury-related morbidity among older adults, with significant healthcare costs and quality-of-life implications~\cite{Bergen2016Falls}. Wearable sensor-based fall detection has emerged as a promising approach for continuous monitoring~\cite{Wu2015Wearable}, and comprehensive surveys have documented the evolution of detection methodologies over the past decade~\cite{Ramachandran2020Survey,mubashir2013survey}. However, deploying such systems on resource-constrained edge devices introduces fundamental trade-offs between detection accuracy and computational efficiency.

Deep learning has transformed sensor-based activity recognition~\cite{wang2019deep}. Convolutional architectures such as ResNet~\cite{he2016resnet} and temporal models including LSTM~\cite{hochreiter1997lstm} have demonstrated strong performance on benchmark datasets. More recently, Transformer-based approaches~\cite{Vaswani2017Attention} have been adapted for time-series tasks, though their effectiveness relative to simpler architectures remains debated~\cite{Zeng2023TransformersEffective}. Hybrid CNN-RNN ensembles have also been explored for fall detection~\cite{Liu2023FallEnsemble,Butt2022FallLSTM}, achieving competitive accuracy but at the cost of increased model complexity.

A common strategy for improving discriminability involves time--frequency representations such as STFT or wavelets~\cite{Zhang2022TFC}, yet these introduce substantial preprocessing overhead that conflicts with real-time edge deployment. The challenge is compounded by alarm fatigue in clinical settings~\cite{Cvach2012AlarmFatigue}, where frequent false positives undermine user trust and system adoption. As detailed in Section~3, removing spectral processing reduces median CPU latency by 31.5\% but decreases TPR at FPR${=}$1\% by 2.73 percentage points, illustrating the accuracy--efficiency trade-off.

From a biomechanical perspective, falls exhibit distinctive kinematic signatures: brief impact transients followed by postural transitions, with characteristic jerk (rate of acceleration change) profiles that differ markedly from activities of daily living~\cite{Kim2024GaitPattern}. This observation motivates time-domain approaches that can directly capture such transient features. Architectural innovations including channel attention~\cite{hu2018squeeze}, spatial-channel attention~\cite{woo2018cbam}, and adaptive kernel selection~\cite{li2019sknet} provide mechanisms for emphasizing salient temporal patterns without explicit frequency-domain transforms.

Self-supervised contrastive learning has shown promise for learning robust representations from unlabeled or weakly-labeled sensor data~\cite{chen2020simclr}. Time-series-specific formulations such as temporal and contextual contrasting~\cite{Eldele2021TSTCC} and time-frequency consistency objectives~\cite{Zhang2022TFC} have achieved state-of-the-art results on activity recognition benchmarks. Data augmentation strategies tailored to time series~\cite{Iwana2021DataAugmentation} and domain adaptation techniques~\cite{Jia2024TransferLearningHAR,Wu2024DomainAdaptationContrastive} further enhance generalization across subjects and sensor configurations.

Building on these foundations, we propose PhyCL-Net, a physics-inspired time-domain network optimized for edge-native fall detection. The architecture centers on two lightweight components:
\begin{enumerate}
\item \textbf{Physics-Guided Dynamic Kernel (PDK):} Adapts temporal receptive fields according to local signal dynamics, enabling efficient multi-scale feature extraction.
\item \textbf{Jerk-aware Fall-Aware Attention (FAA):} Emphasizes high-jerk segments to distinguish falls from vigorous ADLs.
\end{enumerate}

A training-only hierarchical dual-view contrastive objective improves class separability, with auxiliary heads removed during inference to eliminate deployment overhead.

PhyCL-Net achieves 98.21\% ${\pm}$ 0.10\% accuracy and 98.16\% ${\pm}$ 0.10\% Macro-F1 on SisFall with 12-fold LOSO validation. The model contains 1.049M parameters, requires 0.15 GFLOPs, and achieves 125.99\,ms p50 latency (31.5\% reduction vs.\ spectral baseline). At TPR${=}$95\%, FPR is 0.72\%; when FPR is limited to 1\%, TPR decreases by 2.73 percentage points.

The principal contributions are:
\begin{itemize}
\item Deployability as a primary consideration, with parameters, FLOPs, and CPU latency reported alongside accuracy.
\item \textbf{Physics-inspired efficiency:} PDK and FAA utilize physical priors for class discrimination without spectral branches.
\item \textbf{Training-only contrastive regularization:} Zero deployment overhead with auxiliary heads removed after training.
\item \textbf{Safety-oriented analysis:} FPR at TPR${=}$95\% and TPR at FPR${=}$1\% reported for continuous monitoring.
\end{itemize}

The problem is formulated as a window-based time-series classification. Each sample $(X_i, y_i)$ consists of an inertial window $X_i \in \mathbb{R}^{C \times T}$ and binary label $y_i \in \{0,1\}$. Training minimizes class-weighted cross-entropy loss with inverse frequency weighting.

\section{Methodology}

\FloatBarrier
\subsection{Overview of PhyCL-Net}
PhyCL-Net is a compact time-domain backbone comprising a lightweight stem and three stages that progressively downsample the time domain. For input $X \in \mathbb{R}^{B \times 3 \times 512}$, the stem applies GhostConv1d ($k{=}5, s{=}1, p{=}2$), mapping 3 to 48 channels while preserving $T{=}512$. Stage~1 operates at $(C,T){=}(48, 512)$ with two PhyCL-Blocks. Transition layers use SeparableConv1d ($k{=}5, s{=}2, p{=}2$) for downsampling. Stage~2 operates at $(96,256)$ and Stage~3 at $(192,128)$, each with two PhyCL-Blocks. The head uses AdaptiveAvgPool1d(1), Linear (192 to 2), and Softmax for classification.

The model's 1.049M parameters and 0.15 GFLOPs fit within common microcontroller constraints such as the nRF52840's 256\,kB SRAM.

Each PhyCL-Block comprises two processing streams: the Physics-Guided Dynamic Kernel (PDK), which adapts temporal receptive fields to local signal dynamics, and the Jerk-Aware Fall-Aware Attention (FAA), which emphasizes high-jerk intervals. The outputs are integrated using symmetric cross-gated fusion with a residual connection and learnable scaling. Figure~\ref{fig:architecture} illustrates the overall architecture of PhyCL-Net, showing how the input signal flows through the PDK and FAA branches before being fused via the Cross-Gated Fusion Unit. The figure also provides a detailed view of the internal structure of a single PhyCL-Block, highlighting the two parallel processing streams---the Physics-Guided Dynamic Kernel (PDK) for multi-scale temporal adaptation and the Jerk-aware Fall-Aware Attention (FAA) for transient emphasis---and their integration through the symmetric cross-gated fusion unit.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\linewidth]{figures/fig01_architecture_and_block.pdf}
\caption{The overall architecture of PhyCL-Net. The input signal is processed by two parallel branches: the Physics-Guided Dynamic Kernel (PDK) for adaptive receptive fields and the Fall-Aware Attention (FAA) for jerk extraction, fused via the Cross-Gated Fusion Unit. The detailed internal structure of a single PhyCL-Block is also shown, illustrating the two parallel streams (PDK for multi-scale temporal adaptation and FAA for transient emphasis) and their integration through the symmetric cross-gated fusion unit.}
\label{fig:architecture}
\end{figure}

\FloatBarrier

\subsection{Physics-Guided Dynamic Kernel (PDK)}
Falls display mixed temporal dynamics~\cite{Kim2024GaitPattern}. They show an abrupt impact impulse (a sudden, strong signal) followed by slower postural transitions (changes in body movement). Using a single fixed kernel size (window length for convolution operations) is a poor match under tight compute budgets. PDK addresses this by routing features through a small set of depthwise 1D temporal kernels (convolutions focused on individual input channels over time) with sizes $\{7, 15, 31, 63\}$, inspired by selective kernel networks~\cite{li2019sknet}. It then combines their outputs with input-dependent mixture weights (values that determine each kernel's influence).

The mixture weights are predicted from two lightweight signals: (i) a compact 12-dimensional physics descriptor $p=\phi(X)\in\mathbb{R}^{12}$, extracted from the raw input, which includes support vector magnitude (SVM), jerk, jerk rate, and related transient cues, and (ii) a 4-way complexity logit $\ell^{\mathrm{cmp}}$ predicted from global average pooled features. A three-layer physics MLP ($12{\rightarrow}32{\rightarrow}32{\rightarrow}4$) maps $p$ to $w^{\mathrm{phy}}\in\mathbb{R}^{4}$, while a two-layer complexity MLP ($C{\rightarrow}32{\rightarrow}4$) produces $\ell^{\mathrm{cmp}}$. Their element-wise product is normalized via a learnable softmax temperature $\tau_{\mathrm{dks}}=\max(0.1,\tilde{\tau}_{\mathrm{dks}})$ (initialized to 1.0) to obtain the final routing weights $a=\mathrm{Softmax}((\ell^{\mathrm{cmp}}\odot w^{\mathrm{phy}})/\tau_{\mathrm{dks}})$. The PDK output is then computed as:
\begin{equation}
\label{eq:pdk}
\mathrm{PDK}(F) = \sum_{k\in\mathcal{K}} a_k\, \mathrm{DWConv}_{k}(F), \quad \mathcal{K}=\{7,15,31,63\},
\end{equation}
where $\mathrm{DWConv}_{k}$ denotes depthwise 1D convolution with kernel size $k$, $\mathrm{GAP}(\cdot)$ is AdaptiveAvgPool1d(1), and $\odot$ denotes element-wise multiplication. The module remains a single-pass time-domain operation without explicit time--frequency transforms. Figure~\ref{fig:pdk_module} depicts the detailed structure of the PDK module, illustrating how features are routed through multiple temporal kernels with input-dependent mixture weights derived from physics descriptors.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{figures/fig02_pdk_module.pdf}
\caption{Physics-Guided Dynamic Kernel (PDK) module with Dynamic Kernel Selection (DKS). The module routes features through multiple temporal kernels with input-dependent mixture weights derived from physics descriptors.}
\label{fig:pdk_module}
\end{figure}

\subsection{Jerk-aware Fall-Aware Attention (FAA)}
A common failure mode in fall detection is confusing falls with vigorous ADLs (Activities of Daily Living, like sitting or jogging)~\cite{Zafar2025FallTransformer}. FAA injects a kinematics-aligned bias by emphasizing jerk-dominant segments, where rapid temporal change often coincides with the impact phase. The attention mask $M$ is generated through a lightweight pipeline: DWConv($k{=}3$) $\rightarrow$ GroupNorm $\rightarrow$ GELU $\rightarrow$ sigmoid~\cite{woo2018cbam,hu2018squeeze,Wu2018GroupNorm}. The output combines the modulated features with a residual connection:
\begin{equation}
\label{eq:faa}
\mathrm{FAA}(F) = F \odot \sigma\bigl(\mathrm{GELU}(\mathrm{GN}(\mathrm{DWConv}_{3}(F)))\bigr) + F,
\end{equation}
where $\odot$ denotes element-wise multiplication and $\sigma$ is the sigmoid function.

\subsection{Symmetric Cross-Gated Fusion}
\label{sec:fusion}
PDK and FAA provide complementary evidence---multi-scale temporal context versus transient saliency~\cite{hou2021coordinate}. Rather than concatenation, we fuse the two streams with a symmetric cross-gated unit. Let $t=\mathrm{PDK}(F)$ and $f=\mathrm{FAA}(F)$. Each stream modulates the other via multi-scale channel gating with kernels $r\in\{3,5,7\}$: the channel gates are $g_t = \sigma(\sum_{r} \mathrm{Conv}^{\mathrm{ch}}_{r}(\mathrm{GAP}(f)))$ and $g_f = \sigma(\sum_{r} \mathrm{Conv}^{\mathrm{ch}}_{r}(\mathrm{GAP}(t)))$. The gated features $\tilde{F} = t \odot g_t + f \odot g_f$ are further refined by spatial attention $a_s = \sigma(\mathrm{Conv}^{\mathrm{sp}}_{7}([\mathrm{AvgPool}(\tilde{F});\mathrm{MaxPool}(\tilde{F})]))$. The final output is:
\begin{equation}
\label{eq:fusion}
F_{\mathrm{out}} = F + \alpha\,(\tilde{F} \odot a_s),
\end{equation}
where $\mathrm{Conv}^{\mathrm{ch}}_{r}$ denotes a channel-gating operator with kernel size $r$, $\mathrm{Conv}^{\mathrm{sp}}_{7}$ denotes a spatial attention convolution with kernel size 7, and the residual scaling $\alpha$ is learnable and initialized to 0.0. Figure~\ref{fig:cross_gate_fusion} illustrates the symmetric cross-gated fusion unit, showing how each stream modulates the other through multi-scale channel gating and spatial attention to produce a unified representation.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{figures/fig03_cross_gate_fusion.pdf}
\caption{Symmetric Cross-Gated Fusion Unit. Each stream modulates the other through multi-scale channel gating and spatial attention, producing a unified representation that combines multi-scale temporal context from PDK with transient saliency from FAA.}
\label{fig:cross_gate_fusion}
\end{figure}

\subsection{Training-only Hierarchical Dual-view Contrastive Regularization}
To enhance feature separability without incurring deployment overhead, we incorporate a training-only contrastive regularization term based on the InfoNCE objective~\cite{chen2020simclr,Zhang2022TFC}. Six auxiliary projection heads (three stages $\times$ two streams) are attached during training, each comprising global average pooling followed by a linear layer mapping to 128 dimensions. These heads are discarded after training, ensuring zero additional inference cost.

Given a batch of $N$ input windows, we apply two independent stochastic augmentations $t_1(\cdot)$ and $t_2(\cdot)$~\cite{Iwana2021DataAugmentation} to generate paired views. For each sample $X_i$, the backbone $f_\theta$ and projection head $h$ produce $\ell_2$-normalized embeddings:
\begin{equation}
\label{eq:tfcl_embed}
z_i^{(q)}=\frac{h(f_\theta(t_q(X_i)))}{\|h(f_\theta(t_q(X_i)))\|_2}, \quad q\in\{1,2\}.
\end{equation}
The contrastive loss encourages embeddings from the same sample to align while separating those from different samples. Specifically, we adopt a symmetric InfoNCE formulation with cosine similarity $\mathrm{sim}(u,v) = u^\top v$ and temperature $\tau = 0.1$:
\begin{equation}
\label{eq:tfcl_infonce}
\mathcal{L}_{\mathrm{NCE}} = -\frac{1}{2N}\sum_{i=1}^{N}\bigl[\ell_i^{(1\to2)} + \ell_i^{(2\to1)}\bigr],
\end{equation}
where $\ell_i^{(q\to q')}$ denotes the log-softmax probability of correctly identifying the positive pair among all negatives in the batch. The final training objective combines cross-entropy classification loss with the contrastive term:
\begin{equation}
\label{eq:tfcl_total}
\mathcal{L} = \mathcal{L}_{\mathrm{CE}}+\lambda_{\mathrm{tfcl}}\,\mathcal{L}_{\mathrm{NCE}}, \quad \lambda_{\mathrm{tfcl}}= 0.1.
\end{equation}

\subsection{Reproducibility and Deployment Protocol}
We standardize benchmarking and traceability through explicit configuration switches, scripts, and machine-readable artifacts. PhyCL-Net and the spectral baseline share the same model implementation, differing only by a single configuration flag. Each run writes structured outputs including aggregated metrics, fold-level LOSO results, efficiency measurements (FLOPs/parameters/latency), and training logs, enabling direct auditing and re-analysis.

For static complexity, FLOPs and parameter counts are computed with forced CPU execution at input shape $1 \times 3 \times 512$. For runtime performance, latency is measured on the CPU in single-thread mode and reported as p50/p95 at batch sizes 1 and 32 (for throughput comparison), along with the runtime environment. All commands required to reproduce the reported complexity and latency numbers are included in the accompanying code package, and fold-level results are provided as machine-readable artifacts to enable independent verification. All code, configuration files, and artifacts associated with this work are publicly available under the MIT license, facilitating reproducibility and encouraging further research within the community.


\section{Experiments and Results}

\subsection{Dataset and Preprocessing}

Evaluation is conducted on the SisFall benchmark using the widely adopted 12-subject subset (SA01, SA02, SA04, SA05, SA06, SA09, SA10, SA11, SA17, SA18, SA19, SA21) to reflect subject-independent deployment. The raw inertial streams are sampled at 200\,Hz and processed through a fixed pipeline: downsampling to 50\,Hz, 4th-order Butterworth low-pass filtering (5\,Hz cutoff), sliding-window segmentation with a window length of 512 and a stride of 256 (50\% overlap), per-channel z-score normalization, and zero-padding for windows shorter than 512. Unless otherwise specified, the 3-axis accelerometer channels (``accel3'') are used. The resulting evaluation set contains 21,678 windows (12,678 ADL and 9,000 FALL).

\subsection{Evaluation Protocol and Metrics}

We adopt a 12-fold leave-one-subject-out (LOSO) cross-validation scheme. To avoid reporting a favorable single training trajectory, we repeat PhyCL-Net with five random seeds (42, 123, 456, 789, 1024) and report the mean/standard deviation, along with 95\% confidence intervals.

Standard classification metrics assess performance:

\textbf{Accuracy:} Proportion of correct predictions.
\begin{equation}
\label{eq:accuracy}
\mathrm{Accuracy} = \frac{\mathrm{TP} + \mathrm{TN}}{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}}.
\end{equation}

\textbf{Sensitivity (TPR):} Proportion of actual positives correctly identified.
\begin{equation}
\label{eq:sensitivity}
\mathrm{Sensitivity} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}.
\end{equation}

\textbf{Specificity (TNR):} True negative rate for the fall class.
\begin{equation}
\label{eq:specificity}
\mathrm{Specificity} = \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FP}}.
\end{equation}

\textbf{G-Mean:} Geometric mean balancing sensitivity and specificity.
\begin{equation}
\label{eq:gmean}
\mathrm{G\text{-}Mean} = \sqrt{\mathrm{TPR} \times \mathrm{TNR}}.
\end{equation}

\textbf{Macro-F1:} Unweighted mean of class-wise F1 scores.

Continuous monitoring is often constrained by the frequency of false alarms~\cite{Cvach2012AlarmFatigue}; therefore, operating-point metrics that explicitly control false alarms are reported. For instance, a false-positive rate of 0.72\% corresponds to approximately one false alert every two hours. Presenting these percentages as human-scale events facilitates the translation of ROC curve results to real-world adoption, particularly for caregivers monitoring data over extended periods. These metrics were recalculated on the target-domain test set using the LOSO protocol.

\subsection{Training Setup}

Unless stated otherwise, we use a fixed training pipeline for SisFall LOSO. For the reported multi-seed setting, we train for 50 epochs with a batch size of 256, an initial learning rate of 0.004, and 10 warmup epochs. We use AdamW with weight decay $1{\times}10^{-4}$ and a CosineAnnealingLR schedule ($\eta_{\min}{=}1{\times}10^{-6}$). Mixed precision (AMP) is enabled, and gradients are clipped to a max norm of 1.0. The total loss is CE (weight 1.0) plus TFCL (weight 0.1) when enabled, with automatic inverse-frequency class weighting.

\begin{table}[htbp]
\caption{Training hyperparameters for the reported SisFall LOSO experiments.}
\label{tab:train_setup}
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lc}
\toprule
Setting & Value \\
\midrule
Optimizer & AdamW \\
Initial learning rate & 0.004 \\
Epochs & 50 \\
Warmup epochs & 10 \\
Batch size & 256 \\
Weight decay & $1{\times}10^{-4}$ \\
Scheduler & CosineAnnealingLR ($\eta_{\min}{=}1{\times}10^{-6}$) \\
AMP & True \\
Gradient clipping & 1.0 \\
Loss weights & CE: 1.0, TFCL: 0.1 \\
Class weighting & Auto (inverse frequency) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Baselines}

We compare our approach against representative time-series architectures commonly used in wearable sensing, including InceptionTime~\cite{fawaz2020inceptiontime}, a Temporal Convolutional Network (TCN), a lightweight Transformer variant~\cite{Vaswani2017Attention,Zeng2023TransformersEffective}, ResNet~\cite{he2016resnet}, and LSTM~\cite{hochreiter1997lstm}. Recent works have also explored ensemble CNN-RNN hybrids for fall detection~\cite{Liu2023FallEnsemble,Butt2022FallLSTM}. We also include the frequency-heavy in-house baseline MSPA-FAA-PDK, which retains the spectral branch, to isolate the accuracy--efficiency effect of removing spectral processing under an otherwise matched implementation.

\subsection{Main Results (LOSO on SisFall)}

Under LOSO, PhyCL-Net achieves 98.20\% Accuracy and 98.15\% Macro-F1, outperforming TCN (97.13\% Accuracy) and the Transformer baseline (95.48\% Accuracy). Across five seeds, performance remains stable at 98.21\% ${\pm}$ 0.10\% Accuracy and 98.16\% ${\pm}$ 0.10\% Macro-F1. Table~\ref{tab:main_results} summarizes the LOSO performance. Figure~\ref{fig:pareto_tradeoff} visualizes the accuracy--efficiency trade-off, where bubble size is proportional to CPU latency, demonstrating that PhyCL-Net achieves the best accuracy while maintaining competitive efficiency.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig04_pareto_tradeoff.pdf}
    \caption{Accuracy--efficiency trade-off. Bubble size is proportional to single-thread CPU latency (p50). PhyCL-Net (red star) achieves the best accuracy (98.20\%) while maintaining a competitive parameter count (1.049M) and low latency (125.99\,ms).}
    \label{fig:pareto_tradeoff}
\end{figure}

\begin{table}[htbp]
\caption{LOSO performance on SisFall (12 folds). PhyCL-Net values are mean (95\% CI) aggregated across 5 random seeds; other models use 2 seeds with fold-level 95\% CI.}
\label{tab:main_results}
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}lccccc@{}}
\toprule
Model & Params & Acc (\%) & F1 (\%) & Sens (\%) & Spec (\%) \\
\midrule
\textbf{PhyCL-Net (ours)} & \textbf{1.049M} & \textbf{98.21} & \textbf{98.16} & \textbf{97.93} & \textbf{98.41} \\
 & & {\scriptsize(98.09--98.33)} & {\scriptsize(98.04--98.28)} & {\scriptsize(97.80--98.06)} & {\scriptsize(98.27--98.55)} \\
\midrule
MSPA-FAA-PDK & 1.657M & 98.04 & 97.98 & 97.67 & 98.30 \\
\midrule
InceptionTime & 0.041M & 97.91 & 97.85 & 97.82 & 97.97 \\
\midrule
TCN & 0.101M & 97.13 & 97.04 & 96.43 & 97.63 \\
\midrule
Transformer & 0.200M & 95.48 & 95.34 & 94.71 & 96.02 \\
\midrule
ResNet & 0.014M & 95.13 & 94.98 & 94.41 & 95.64 \\
\midrule
LSTM & 0.532M & 95.02 & 94.86 & 94.35 & 95.50 \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:fold_stability} presents the cross-validation stability analysis, displaying the fold-wise performance comparison across 12 subjects. The results indicate that PhyCL-Net consistently outperforms the baseline in nearly every fold, exhibiting lower variance and greater stability across subjects.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{figures/fig05_fold_stability.pdf}
\caption{\textbf{Cross-Validation Stability Analysis.} The fold-wise performance comparison across 12 subjects demonstrating that PhyCL-Net (Ours) consistently outperforms the baseline in almost every fold, exhibiting lower variance and higher stability.}
\label{fig:fold_stability}
\end{figure}

Figure~\ref{fig:radar} provides a multi-metric comparison among the top models through a radar chart, allowing visual assessment of performance across multiple dimensions simultaneously. Figure~\ref{fig:tsne} visualizes the feature embeddings using t-SNE, demonstrating the discriminative power of PhyCL-Net's learned representations, where fall and ADL samples form clearly separated clusters. Figure~\ref{fig:attention} shows the attention visualization of the FAA module, highlighting how the attention weights focus on jerk-dominant segments corresponding to impact phases. Finally, Figure~\ref{fig:confusion} presents the row-normalized confusion matrix, providing a detailed view of the classification performance across both classes. Recent advances in graph neural networks for time series~\cite{Jin2024GNNTimeSeries} suggest potential future directions for modeling inter-channel dependencies.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/fig06_radar.pdf}
    \caption{Radar chart comparing PhyCL-Net, MSPA-FAA-PDK, and TCN across four metrics. Axes are scaled from 95\% to 100\% to highlight differences.}
    \label{fig:radar}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/fig07_tsne.pdf}
    \caption{t-SNE visualization of learned embeddings. Fall and ADL samples form separated clusters under PhyCL-Net, consistent with improved class separation in the learned feature space.}
    \label{fig:tsne}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig08_attention.pdf}
    \caption{Attention visualization of the Fall-Aware Attention (FAA) module. The attention weights highlight jerk-dominant segments corresponding to impact phases, demonstrating the module's ability to focus on kinematically relevant regions.}
    \label{fig:attention}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/fig09_confusion.pdf}
    \caption{Row-normalized confusion matrix for PhyCL-Net under 12-fold LOSO evaluation, derived from the 5-seed aggregated Sensitivity (97.93\%) and Specificity (98.41\%) to remain consistent with the headline metrics.}
    \label{fig:confusion}
\end{figure}


\subsection{Edge-Oriented Efficiency Benchmarks}
\label{sec:efficiency}

We report both static complexity and measured runtime under a fixed protocol. PhyCL-Net contains 1.049M parameters and requires 0.150979 GFLOPs (fvcore; input $1{\times}3{\times}512$). FLOPs and parameter counts are computed with forced CPU execution.

Runtime latency is measured on the CPU in single-thread mode and summarized with p50/p95 statistics. On the reported software environment (PyTorch 2.5.1, Python 3.10.11, Windows 10), PhyCL-Net achieves 125.99\,ms p50 / 141.43\,ms p95 at batch size 1, compared with 184.31\,ms p50 / 203.27\,ms p95 for MSPA-FAA-PDK, corresponding to a 31.5\% reduction in p50 latency. Table~\ref{tab:complexity} details the comparison.

\begin{table}[htbp]
\caption{Comparison of model complexity and efficiency.}
\label{tab:complexity}
\centering
\begin{tabular}{lcccc}
\toprule
Model & Params (M) & FLOPs (G) & Latency p50 (ms) & Latency p95 (ms) \\
\midrule
MSPA-FAA-PDK (baseline) & 1.657 & 0.202923 & 184.31 & 203.27 \\
\textbf{PhyCL-Net (ours)} & \textbf{1.049} & \textbf{0.150979} & \textbf{125.99} & \textbf{141.43} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Operating-point Reliability (False-alarm Control)}

At TPR${=}$95\%, PhyCL-Net achieves the lowest FPR among all evaluated models (0.72\% vs.\ 0.82--4.19\% for baselines), reducing the window-level false-alarm rate by up to 83\% compared to the Transformer baseline. This lower base false-alarm rate enables downstream smoothing or multi-window confirmation to further reduce alerts. Under a strict low-false-alarm constraint (FPR${=}$1\%), removing the spectral branch reduces TPR from 96.02\% to 93.29\% ($-$2.73\,pp), while TPR@FPR${=}$5\% remains at 99.26\%. Table~\ref{tab:safety} reports the operating-point trade-offs.

\begin{table}[htbp]
\caption{Safety-oriented metrics demonstrating the Accuracy-Efficiency Trade-off.}
\label{tab:safety}
\centering
\small
\begin{tabular}{lccc}
\toprule
Model & TPR@FPR=1\% (\%) & TPR@FPR=5\% (\%) & FPR@TPR=95\% (\%) \\
\midrule
MSPA-FAA-PDK (baseline) & \textbf{96.02} & 99.28 & 0.82 \\
InceptionTime & 95.52 & 99.11 & 0.90 \\
\textbf{PhyCL-Net (ours)} & 93.29 & \textbf{99.26} & \textbf{0.72} \\
TCN & 93.38 & 98.12 & 1.38 \\
Transformer & 82.98 & 95.86 & 4.19 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}

Component-wise ablations offer detailed insights into the nuanced contributions of each module under various operating conditions. Here, we summarize the ablation results for each module to elucidate their specific roles. Disabling dynamic kernel selection within the physics-guided kernel module resulted in essentially unchanged accuracy (98.22\%${\pm}$0.08, $n{=}2$), indicating insensitivity to the design choice regarding accuracy, but notably increased CPU detection latency to 213.3\,ms. This suggests that the adaptive kernel routing substantially enhances efficiency under our benchmark protocol. Removing the training-only contrastive regularizer (TFCL) resulted in a modest decrease in accuracy to 97.96\%${\pm}$0.03 ($n{=}2$). This outcome aligns with the premise that TFCL improves representation separability during training, enhancing model performance without adding deployment overhead. Furthermore, eliminating the fall-aware attention (FAA) component yielded a comparable aggregate accuracy of 98.24\%${\pm}$0.12 ($n{=}2$), indicating that it primarily serves as an inductive bias for challenging cases rather than substantially boosting headline accuracy. These results highlight the importance of each module in striking a balance between efficiency and performance, particularly under varying operating conditions. Table~\ref{tab:ablation} summarizes the key design variants, and Table~\ref{tab:detailed_ablation} provides a detailed component-wise ablation study.

\begin{table}[htbp]
\caption{Ablation summary under SisFall LOSO.}
\label{tab:ablation}
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lccccc}
\toprule
Variant & Params (M) & Acc (\%) & Macro-F1 (\%) & TPR@FPR=1\% (\%) & CPU p50 (ms) \\
\midrule
MSPA-FAA-PDK ($n{=}2$) & 1.657 & 98.04 & 97.98 & \textbf{96.02} & 184.31 \\
w/o TFCL ($n{=}2$) & 1.657 & 97.96$\pm$0.03 & 97.90$\pm$0.03 & 95.45 & 181.33 \\
\textbf{PhyCL-Net ($n{=}5$)} & \textbf{1.049} & \textbf{98.21$\pm$0.10} & \textbf{98.16$\pm$0.10} & 93.29 & \textbf{125.99} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\caption{Component-wise ablation study on SisFall LOSO. Ablations with $n{=}2$ seeds are indicative; the main model uses $n{=}5$.}
\label{tab:detailed_ablation}
\centering
\small
\begin{tabular}{lcccc}
\toprule
Configuration & Acc (\%) & Macro-F1 (\%) & Latency (ms) & Seeds \\
\midrule
\textbf{PhyCL-Net (complete)} & \textbf{98.21$\pm$0.10} & \textbf{98.16$\pm$0.10} & 125.99 & $n{=}5$ \\
\midrule
$-$ DKS (fixed kernel) & 98.22$\pm$0.08 & 98.18$\pm$0.09 & 213.3 & $n{=}2$ \\
$-$ FAA (no Jerk attention) & 98.24$\pm$0.12 & 98.19$\pm$0.12 & 120.15 & $n{=}2$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Reproducibility Note.} Each run exports machine-readable artifacts, including aggregated metrics, fold-level LOSO results, efficiency measurements, and training logs, enabling fold-level auditing and independent re-analysis of both accuracy and efficiency results.


\section{Discussion}

The experimental results demonstrate the effectiveness of the proposed PhyCL-Net method for edge-native fall detection. PhyCL-Net builds on physics-inspired design principles by introducing several critical enhancements that, together, create a practical framework for resource-constrained deployment.

Unlike standard approaches that rely on computationally expensive spectral processing, PhyCL-Net integrates physics-guided components directly into the time-domain backbone. This design ensures efficient inference while maintaining strong discriminative capability, a crucial choice validated by ablation studies.

Furthermore, the method employs an adaptive, instance-specific kernel selection mechanism and a jerk-aware attention strategy. This allows for more granular temporal adaptation compared to fixed-kernel schemes, addressing the specific dynamics of fall events on a per-sample basis.

The integration of training-only contrastive regularization further refines the model's representation learning. This combined approach provides a more effective framework for addressing challenging edge-deployment scenarios than existing methods.

\subsection{Edge-Relevant Gains and Operating Points}

Compared with the spectral baseline, p50 CPU latency decreases from 184.31\,ms to 125.99\,ms (p95: 203.27\,ms${\rightarrow}$141.43\,ms) under single-thread execution. At TPR${=}$95\%, PhyCL-Net achieves FPR${=}$0.72\%, thereby reducing the false-alarm burden in always-on monitoring. Subject-independent accuracy remains comparable across variants (Wilcoxon $p{=}0.47$ on fold-level aggregate accuracy).

The progression in Table~\ref{tab:main_results} shows incremental benefits across model architectures. LSTM and ResNet suffer from limited temporal modeling capacity. TCN and InceptionTime offer modest gains through multi-scale convolutions. The Transformer baseline improves with attention mechanisms, but at a higher computational cost. PhyCL-Net's superior performance stems from its synergistic integration of:
\begin{enumerate}
\item \textbf{Physics-guided temporal adaptation:} PDK provides multi-scale receptive fields guided by kinematic descriptors.
\item \textbf{Transient-aware attention:} FAA emphasizes jerk-dominant segments to reduce fall--ADL confusion.
\item \textbf{Efficient feature fusion:} The symmetric cross-gated unit combines complementary evidence without concatenation overhead.
\item \textbf{Representation regularization:} Training-only contrastive learning improves separability without deployment cost.
\end{enumerate}

\subsection{Accuracy Impact of Removing the Spectral Branch}

Operating-point analysis shows that spectral processing can still contribute under very strict low-false-alarm constraints: at FPR${=}$1\%, TPR decreases by 2.73\,pp when MSPA is removed (96.02\%${\rightarrow}$93.29\%). At the same time, paired LOSO testing reveals no significant difference in aggregate accuracy between the two variants (Wilcoxon $p{=}0.47$), suggesting that the time-domain backbone captures most discriminative information once it is structured around transient kinematics.

\subsection{Why Time-Domain Modeling Works in This Setting}

Falls are typically characterized by a brief impact transient and a short transition phase, whereas many ADLs exhibit more regular temporal structure. PDK provides multi-scale temporal adaptation within a lightweight computation graph, allowing receptive fields to adjust to local dynamics. FAA complements this by emphasizing jerk-dominant segments, reducing the risk that impact evidence is diluted when features are aggregated over a full window. Together, these components encode a physics-aligned inductive bias that remains compatible with edge constraints.

\subsection{Robustness and Practical Margins}

To assess robustness to sensor noise, additive white Gaussian noise (AWGN) was injected into the accelerometer stream, with SNR swept from 40 to 5\,dB. Signal power was defined as the variance of the demeaned signal to avoid DC/gravity bias in SNR computation. Using a held-out subject (SA01) and repeating each SNR point 5 times, performance degraded smoothly as noise increased: accuracy remained near the clean level at 40--35\,dB (98.04\%${\pm}$0.03 and 97.40\%${\pm}$0.07), dropped at 30\,dB (91.49\%${\pm}$0.15), and declined sharply below 25\,dB. The very small standard deviations across SNR points ($<$0.15\%) indicate stable behavior across noise realizations. These findings suggest a practical robustness margin under typical wearable sensing conditions (approximately 30--40\,dB), while identifying the regime ($<$25\,dB) where sensing quality, rather than model capacity, dominates errors.

Robustness to motion artifacts and long-term sensor drift was not evaluated in this study. These factors are crucial in real-world deployments, where sensors may encounter various types of interference or experience performance degradation over time. Motion artifacts, such as those arising from abrupt user movements, can lead to false positives or missed falls. Long-term sensor drift may gradually alter system sensitivity, reducing reliability. Future research will explicitly model these factors, including spike-like transients and low-frequency bias. It will report the resulting changes in TPR at low-FPR operating points to ensure the system maintains accuracy and effectiveness in diverse operational conditions.

\subsection{Limitations and Future Directions}

The current results are limited to the SisFall dataset. To strengthen external validity, we will extend the evaluation across additional datasets and populations under the same subject-independent protocol~\cite{Jia2024TransferLearningHAR,Wu2024DomainAdaptationContrastive}. Preliminary evaluations for microcontroller deployment using INT8 quantization have not been conducted yet~\cite{Chu2024PostTrainingQuantization,Zhang2023DiffQuant,Gheorghe2021ModelBasedQuantization}. However, we anticipate challenges, including potential trade-offs between model precision and deployment efficiency, as well as the need to balance computational load with battery life in resource-constrained environments~\cite{Yu2023EdgeOptimization,Benoit2024PreImpactFall}. These aspects will be crucial to address in order to achieve operational readiness in real-world applications.

\subsection{Advantages}

The proposed PhyCL-Net method offers several key advantages:
\begin{itemize}
\item \textbf{Deployment efficiency:} The time-domain design eliminates spectral preprocessing overhead, reducing p50 latency by 31.5\% while maintaining competitive accuracy.
\item \textbf{Physics-aligned inductive bias:} PDK and FAA encode kinematic priors that improve discrimination of transient fall signatures without additional computational cost.
\item \textbf{Low false-alarm burden:} At TPR${=}$95\%, PhyCL-Net achieves FPR${=}$0.72\%, enabling practical continuous monitoring with reduced alert fatigue.
\item \textbf{Reproducible benchmarking:} The standardized protocol with machine-readable artifacts enables independent verification and fair comparison.
\item \textbf{Modular design:} The architecture is flexible and extensible for various edge deployment scenarios and hardware constraints.
\end{itemize}

\subsection{Limitations}

Despite its advantages, PhyCL-Net has several limitations:
\begin{itemize}
\item \textbf{Single-dataset evaluation:} Results are limited to SisFall; cross-dataset generalization requires further validation.
\item \textbf{Strict low-FPR trade-off:} At FPR${=}$1\%, TPR decreases by 2.73\,pp compared to the spectral baseline, indicating spectral features may still benefit very strict false-alarm constraints.
\item \textbf{Noise sensitivity:} Performance degrades sharply below 25\,dB SNR, where sensing quality dominates errors.
\item \textbf{Unexplored factors:} Robustness to motion artifacts and long-term sensor drift were not evaluated.
\item \textbf{Hardware validation pending:} Microcontroller-class deployment with INT8 quantization remains future work.
\end{itemize}

This research introduces PhyCL-Net, a physics-guided time-domain network for edge-native wearable fall detection that addresses latency, memory, and energy constraints while maintaining high sensitivity. PhyCL-Net integrates three key contributions:

\begin{enumerate}
\item \textbf{Physics-Guided Dynamic Kernel (PDK)} for adaptive temporal receptive fields that respond to local signal dynamics without explicit spectral transforms.
\item \textbf{Jerk-aware Fall-Aware Attention (FAA)} combined with symmetric cross-gated fusion for emphasizing transient impact signatures.
\item \textbf{Training-only hierarchical dual-view contrastive regularization} to improve representation separability with zero deployment overhead.
\end{enumerate}

Experimental evaluations on the SisFall benchmark under 12-fold LOSO validation demonstrated PhyCL-Net's effectiveness. The model achieved:
\begin{itemize}
\item 98.21\%${\pm}$0.10\% accuracy and 98.16\%${\pm}$0.10\% Macro-F1 across five seeds
\item 1.049M parameters and 0.15 GFLOPs (input $1{\times}3{\times}512$)
\item 31.5\% reduction in single-thread CPU p50 latency (125.99\,ms vs.\ 184.31\,ms)
\item FPR${=}$0.72\% at TPR${=}$95\% for deployment-relevant operating points
\end{itemize}

Ablation studies confirmed the contributions of each core component, with the physics-guided design providing both accuracy and efficiency benefits. The standardized benchmarking protocol with machine-readable artifacts enables independent verification.

Limitations include single-dataset evaluation and unexplored robustness to motion artifacts. Future research directions include extending the evaluation to additional datasets, developing INT8 quantization for microcontroller-class deployment, and exploring hardware-aware optimization through neural architecture search~\cite{Heuillet2024DNAS,Benmeziane2024MedicalNAS}.

\vspace{1em}
\noindent\textbf{Author contributions:} Conceptualisation, methodology, software, validation, formal analysis, investigation, resources, data curation, writing---original draft, visualisation: Anonymous Author(s). All authors have read and agreed to the published version of the manuscript.

\vspace{0.5em}
\noindent\textbf{Funding:} This research received no external funding.

\vspace{0.5em}
\noindent\textbf{Data availability statement:} The datasets used in this study, SisFall~\cite{sucerquia2017sisfall}, are publicly available. The source code, derived experimental results, figures, and trained model checkpoints supporting the findings of this study are available at: \texttt{https://github.com/AlexZander-666/1}. Model checkpoints are provided as GitHub Release assets together with SHA-256 checksums for integrity verification.

\vspace{0.5em}
\noindent\textbf{Conflicts of interest:} The authors declare no conflict of interest.

\vspace{0.5em}
\noindent\textbf{Declaration on generative AI:} Semantic Scholar and Google Scholar were used for literature discovery. Claude Opus 4.5 assisted with drafting, structure, and terminology consistency; Grammarly was used for grammar and spelling. The authors manually verified all technical claims, references, and experimental results. The authors take full responsibility for the content and accuracy of this publication.

\bibliography{main}

\end{document}
